{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse job offer text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting text from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "---------------------------------------------\n",
      "---------------Job Description---------------\n",
      "---------------------------------------------\n",
      "100%, Zurich, temporaryThe Chair of Systems Design at ETH Zurich invites applications for two PhD positions in the field of Computational Social Sciences. The first PhD position focuses on the analysis of political speeches using NLP approaches. The second PhD position focuses on quantifying resilience in collaboration networks. Both PhD positions leverage a new data set on the Swiss parliament longitudinal, 1891 until today . The two positions are associated with our SNF project Analyzing Co-Sponsorship Networks from 127 Years of the Swiss Federal Assembly , but successful candidates are required to develop their own research plan. Project background The two PhD positions are loosely associated with our SNF project. In this project we aim to detect when politicians change their opinions and why. We examine parliamentary speeches using both supervised and unsupervised ML approaches and the temporal dynamics of how members of parliament work together. Our project builds on a newly compiled data set of the Swiss parliament from 1891-today. It contains all speeches, political interactions and collaboration networks from members of parliament over the past 128 years. For more information, see .Job DescriptionOur multi-lingual data set offers the opportunity to develop new methodologies in the field of NLP including e.g., analyzing evolving speech patterns, speech complexity and sentiment analysis and network science including e.g., analyzing collaboration networks, multi-level networks, developing inferential network models . Starting from this unique data set, prospective candidates will develop their own research interests. The positions are available from May 2020 flexible and are fully funded for 3+ years by the Swiss National Science Foundation SNF . Deadline for applications is February 16, 2020. Your profile PhD Position With Focus On NLP MSc degree in computer science or related field Strong analytical skills and background in natural language processing As most of the data is in German, good German skills are required PhD Position With Focus On Data Science And Networks MSc degree in physics, data science, computer science alternatively MSc MA in social science with exceptional skills in quantitative analysis strong background in data analysis and modelling excellent programming skills and ideally also high-performance computing and machine learning database management e.g., SQL, neo4j As most of the data is in German, good German skills are required Apart from excellent technical skills, both candidates should show an interest in social and political science questions, be able to communicate well in an interdisciplinary team, and be highly motivated and creative in what they do. ETH Zurich ETH Zurich is one of the world’s leading universities specialising in science and technology. We are renowned for our excellent education, cutting-edge fundamental research and direct transfer of new knowledge into society. Over 30,000 people from more than 120 countries find our university to be a place that promotes independent thinking and an environment that inspires excellence. Located in the heart of Europe, yet forging connections all over the world, we work together to develop solutions for the global challenges of today and tomorrow. Interested?We look forward to receiving your online application with the following documents: CV, motivation letter including detailed description of relevant skills and your interest in the above mentioned research project and transcript of grades. Please apply exclusively via the online application portal. Applications sent by email or post will not be considered.Further information about the Chair of Systems Design can be found on our website . For specific inquiries, please contact Prof. Dr. Frank Schweitzer at click apply no applications .Seniority levelAssociateEmployment typeTemporaryJob functionResearchAnalystInformation TechnologyIndustriesInformation Technology and ServicesResearchStaffing and Recruiting\n"
     ]
    }
   ],
   "source": [
    "# !pip install selenium\n",
    "from selenium import webdriver \n",
    "import os\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "url = \"https://www.linkedin.com/jobs/view/1732636707/\"\n",
    "# url = 'https://www.linkedin.com/jobs/view/1489206302/'\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.dirname('__file__'))\n",
    "DRIVER_BIN = os.path.join(PROJECT_ROOT, \"chromedriver\")\n",
    "\n",
    "driver = webdriver.Chrome(executable_path = DRIVER_BIN)\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "driver.implicitly_wait(20) # seconds\n",
    "description = driver.find_element_by_class_name('description')\n",
    "\n",
    "\n",
    "\n",
    "elementHTML = description.get_attribute('innerHTML')\n",
    "elementHTML = BeautifulSoup(elementHTML)\n",
    "\n",
    "text = elementHTML.get_text()\n",
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace('(', ' ')\n",
    "text = text.replace(')', ' ')\n",
    "text = text.replace('/', ' ')\n",
    "text = re.sub(' +',' ', text)\n",
    "\n",
    "\n",
    "\n",
    "#input('Press ENTER to close the automated browser')\n",
    "\n",
    "driver.quit()\n",
    "print('done')\n",
    "print('---------------------------------------------')\n",
    "print('---------------Job Description---------------')\n",
    "print('---------------------------------------------')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press ENTER to close the automated browser\n",
      "done\n",
      "\n",
      "\n",
      "---------------title---------------\n",
      "\n",
      "\n",
      "Machine Learning Scientist\n",
      "\n",
      "\n",
      "---------------company---------------\n",
      "\n",
      "\n",
      "ASOS.com\n",
      "\n",
      "\n",
      "---------------location---------------\n",
      "\n",
      "\n",
      "London, England, United Kingdom\n",
      "\n",
      "\n",
      "---------------time---------------\n",
      "\n",
      "\n",
      "2 weeks ago\n",
      "\n",
      "\n",
      "---------------num_app---------------\n",
      "\n",
      "\n",
      "Over 200 applicants\n",
      "\n",
      "\n",
      "---------------description---------------\n",
      "\n",
      "\n",
      "ASOS is one of the UK’s top fashion and beauty destinations, expanding globally at a rapid pace. Our values are to be authentic, brave and creative, and we live and breathe these in everything we do.We believe fashion can make you look, feel and be your best and, with technology in our DNA, we deliver the latest trends to our digital-obsessed 20-something market. Our award winning Tech teams sit at the heart of our business. We deliver technical innovation and pioneer incredible solutions, which are crucial to our continued success. We’re extremely ambitious and thrive on the individuality of our amazing employees. Our values encompass everything needed for our tech people to be the thought leaders of tomorrow.We are looking for Machine Learning Scientists to join our team and play a key role in helping ASOS provide the best shopping experience to our millions of customers. The role offers broad exposure to ASOS, requiring close collaboration with retail, marketing and technology divisions. You will be part of a highly innovative AI platform working alongside engineers and fellow scientists to solve and productionise interesting and difficult problems and leveraging cutting edge technology. At ASOS, as an online only retailer, we have unique datasets – transactions and click streams for millions of customers and photos, videos, and text descriptions of hundreds of thousands of products.The ideal candidate will have a strong technical background and experience solving tough problems with large datasets. You will be a highly intelligent self-starter, able to work independently with a strong attention to detail. What you'll be doing... Working in cross functional team, alongside engineers, b usiness analysts and non-technical stakeholders, creat ing new and impro ving internal and external facing data products Driv ing the implementation and scale-up of algorithms for measurable impact across the business Setting up and conduct ing large - scale experiments to test hypotheses and drive product development. Keeping up with relevant state-of-the-art research, taking part in reading groups alongside other scientists, with the opportunity to publish novel prototypes for the business at top conferences We'd love to meet someone with... An advanced degree in Computer Science, Physics, Mathematics or a similar quantitative subject - a Ph.D. is a bonus Experience in using machine learning methods to solve problems involving complex high - dimensional data e.g. image, click - streams, text, video, speech, time series - This can either be through a distinguished academic career alongside relevant publications, or significant experience solving and productionising models within industry An understanding of the retail , marketing, and or ecommerce industry Comfortable working in a Python data science tech stack e.g. pandas, N um P y, Dask , scikit -learn, K eras , PySpark , P y T orch . Additional knowledge of Scala is desirable Experience accessing and combining data from multiple sources and building data pipelines , including a g ood knowledge of SQL Understanding of software development lifecycles and engineering practices Data pipelines, API workflows, CI CD deployments alongside ML DataOps , MLOps A solid understanding of statistics hypothesis testing, regressions, random variables, inference The ability to work collaboratively and proactively in a fast - paced environment alongside both scientists, engineers and non-technical stakeholders A ‘hackers’ mentality , comfortable using open source technologies.\n",
      "\n",
      "\n",
      "---------------job_criteria---------------\n",
      "\n",
      "\n",
      "Seniority levelNot ApplicableEmployment typeFull-timeJob functionOtherIndustriesApparel & FashionInternetRetail\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "# url = \"https://www.linkedin.com/jobs/view/1732636707/\"\n",
    "url = 'https://www.linkedin.com/jobs/view/1489206302/'\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.dirname('__file__'))\n",
    "DRIVER_BIN = os.path.join(PROJECT_ROOT, \"chromedriver\")\n",
    "\n",
    "driver = webdriver.Chrome(executable_path = DRIVER_BIN)\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "driver.implicitly_wait(20) # seconds\n",
    "title = driver.find_element_by_class_name('topcard').find_element(By.XPATH, '//h1[contains(@class, \"topcard__title\")]')\n",
    "company = driver.find_element_by_class_name('topcard').find_element(By.XPATH, '//span[contains(@class, \"topcard__flavor\")]')\n",
    "location = driver.find_element_by_class_name('topcard').find_element(By.XPATH, '//span[contains(@class, \"topcard__flavor topcard__flavor--bullet\")]')\n",
    "time = driver.find_element_by_class_name('topcard').find_element(By.XPATH, '//span[contains(@class, \"topcard__flavor--metadata posted-time-ago__text\")]')\n",
    "num_app = driver.find_element_by_class_name('topcard').find_element(By.XPATH, '//figcaption[contains(@class, \"num-applicants__captio\")]')\n",
    "\n",
    "description = driver.find_element_by_class_name('description').find_element(By.XPATH, '//div[contains(@class, \"description__text description__text--rich\")]')\n",
    "job_criteria = driver.find_element_by_class_name('description').find_element(By.XPATH, '//ul[contains(@class, \"job-criteria__list\")]')\n",
    "\n",
    "data = dict()\n",
    "data.update({'title' : title , 'company' : company , 'location' : location , 'time' : time \n",
    "             , 'num_app' : num_app ,'description' : description , 'job_criteria' : job_criteria })\n",
    "\n",
    "linkedIn = dict()\n",
    "\n",
    "for key, value in data.items():\n",
    "    elementHTML = value.get_attribute('innerHTML')\n",
    "    elementHTML = BeautifulSoup(elementHTML)\n",
    "    text = elementHTML.get_text()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = text.replace('/', ' ')\n",
    "    text = re.sub(' +',' ', text)\n",
    "    linkedIn[key] = text\n",
    "\n",
    "\n",
    "\n",
    "input('Press ENTER to close the automated browser')\n",
    "\n",
    "driver.quit()\n",
    "print('done')\n",
    "# print('---------------------------------------------')\n",
    "# print('---------------Job Description---------------')\n",
    "# print('---------------------------------------------')\n",
    "for key, value in linkedIn.items():\n",
    "    print('\\n')\n",
    "    print('---------------{}---------------'.format(key))\n",
    "    print('\\n')\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to C:\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download nltk attribute if needed\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "filtered_text = [word for word in filtered_text if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = [''.join(c for c in s if c not in string.punctuation) for s in filtered_text]\n",
    "filtered_text = [s for s in filtered_text if s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = [word.lower() for word in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "filtered_text_stem = [stemmer.stem(t) for t in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "filtered_text_lem = [lemmatizer.lemmatize(t) for t in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------head words-----------------\n",
      "                             word  count\n",
      "0                       seniority      1\n",
      "1                        levelnot      1\n",
      "2            applicableemployment      1\n",
      "3                 typefulltimejob      1\n",
      "4  functionotherindustriesapparel      1\n",
      "5           fashioninternetretail      1\n",
      "-----------------tail words-----------------\n",
      "                             word  count\n",
      "0                       seniority      1\n",
      "1                        levelnot      1\n",
      "2            applicableemployment      1\n",
      "3                 typefulltimejob      1\n",
      "4  functionotherindustriesapparel      1\n",
      "5           fashioninternetretail      1\n"
     ]
    }
   ],
   "source": [
    "#nltkText = [filtered_text_stem, filtered_text_lem]\n",
    "\n",
    "fdist = nltk.FreqDist(filtered_text_lem)\n",
    "\n",
    "fdistDF = pd.DataFrame.from_dict(fdist, orient='index').reset_index()\n",
    "fdistDF = fdistDF.rename(columns={'index':'word', 0:'count'})\n",
    "fdistDF = fdistDF.sort_values(by='count', ascending=False)\n",
    "print('-----------------head words-----------------')\n",
    "print(fdistDF.head(10).reset_index(drop=True))\n",
    "print('-----------------tail words-----------------')\n",
    "print(fdistDF.tail(10).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze document words with data science job descripiton database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To indentify what skills are specific for this job vacancy in particulair we have to create a data science job description database. Using this database we can identify what is specific about this job with the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Term-Document Matrix\n",
    "\n",
    "Use scikit-learn's <code>TfidfVectorizer</code> class to construct a [term-document matrix](http://en.wikipedia.org/wiki/Document-term_matrix) containing the TF-IDF score for each word in each document in the data science job description database. In essence, the rows of this sparse matrix correspond to documents in the corpus, the columns represent each word in the vocabulary of the corpus, and each cell contains the TF-IDF value for a given word in a given document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Scores\n",
    "\n",
    "Now that we've built the term-document matrix, we can explore its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDM contains 7 terms and 7 documents\n",
      "first term: applicableemployment\n",
      "last term: typefull\n",
      "random term: levelnot\n",
      "random term: timejob\n",
      "random term: timejob\n",
      "random term: fashioninternetretail\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', analyzer='word', lowercase=True)\n",
    "content = text.split()\n",
    "tfidf_vector = tfidf.fit_transform(content)\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(content)\n",
    "\n",
    "tdm= pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "print ('TDM contains ' + str(len(feature_names)) + ' terms and ' + str(tdm.shape[0]) + ' documents')\n",
    "\n",
    "print ('first term: ' + feature_names[0])\n",
    "print ('last term: ' + feature_names[len(feature_names) - 1])\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print ('random term: ' + feature_names[randint(1,len(feature_names) - 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the Summary\n",
    "\n",
    "That's all we'll need to produce a summary for any document in the corpus. In the example code below, we start by randomly selecting an article from the data science job description database. We iterate through the article, calculating a score for each sentence by summing the TF-IDF values for each word appearing in the sentence. We normalize the sentence scores by dividing by the number of tokens in the sentence (to avoid bias in favor of longer sentences). Then we sort the sentences by their scores, and return the highest-scoring sentences as our summary. The number of sentences returned corresponds to roughly 20% of the overall length of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SUMMARY ***\n",
      "THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\n",
      "  Thailand's trade deficit widened to 4.5\n",
      "  billion baht in the first quarter of 1987 from 2.1 billion a\n",
      "  year ago, the Business Economics Department said.\n",
      "It said Janunary/March imports rose to 65.1 billion baht\n",
      "  from 58.7 billion.\n",
      "\n",
      "*** ORIGINAL ***\n",
      "THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\n",
      "  Thailand's trade deficit widened to 4.5\n",
      "  billion baht in the first quarter of 1987 from 2.1 billion a\n",
      "  year ago, the Business Economics Department said.\n",
      "      It said Janunary/March imports rose to 65.1 billion baht\n",
      "  from 58.7 billion. Thailand's improved business climate this\n",
      "  year resulted in a 27 pct increase in imports of raw materials\n",
      "  and semi-finished products.\n",
      "      The country's oil import bill, however, fell 23 pct in the\n",
      "  first quarter due to lower oil prices.\n",
      "      The department said first quarter exports expanded to 60.6\n",
      "  billion baht from 56.6 billion.\n",
      "      Export growth was smaller than expected due to lower\n",
      "  earnings from many key commodities including rice whose\n",
      "  earnings declined 18 pct, maize 66 pct, sugar 45 pct, tin 26\n",
      "  pct and canned pineapples seven pct.\n",
      "      Products registering high export growth were jewellery up\n",
      "  64 pct, clothing 57 pct and rubber 35 pct.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from nltk.corpus import reuters\n",
    "import math\n",
    "\n",
    "def tokenize_and_stem(sentence):\n",
    "    filtered_text = [word for sent in nltk.sent_tokenize(sentence) for word in nltk.word_tokenize(sent)]\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    filtered_text_stem = [stemmer.stem(t) for t in filtered_text]\n",
    "    return filtered_text_stem\n",
    "    \n",
    "    \n",
    "    \n",
    "article_id = randint(0, tdm.shape[0] - 1)\n",
    "article_text = reuters.raw(reuters.fileids()[article_id])\n",
    "# article_text = \"100%, Zurich, temporaryThe Chair of Systems Design at ETH Zurich invites applications for two PhD positions in the field of Computational Social Sciences. The first PhD position focuses on the analysis of political speeches using NLP approaches. The second PhD position focuses on quantifying resilience in collaboration networks. Both PhD positions leverage a new data set on the Swiss parliament longitudinal, 1891 until today . The two positions are associated with our SNF project Analyzing Co-Sponsorship Networks from 127 Years of the Swiss Federal Assembly , but successful candidates are required to develop their own research plan. Project background The two PhD positions are loosely associated with our SNF project. In this project we aim to detect when politicians change their opinions and why. We examine parliamentary speeches using both supervised and unsupervised ML approaches and the temporal dynamics of how members of parliament work together. Our project builds on a newly compiled data set of the Swiss parliament from 1891-today. It contains all speeches, political interactions and collaboration networks from members of parliament over the past 128 years. For more information, see .Job DescriptionOur multi-lingual data set offers the opportunity to develop new methodologies in the field of NLP including e.g., analyzing evolving speech patterns, speech complexity and sentiment analysis and network science including e.g., analyzing collaboration networks, multi-level networks, developing inferential network models . Starting from this unique data set, prospective candidates will develop their own research interests. The positions are available from May 2020 flexible and are fully funded for 3+ years by the Swiss National Science Foundation SNF . Deadline for applications is February 16, 2020. Your profile PhD Position With Focus On NLP MSc degree in computer science or related field Strong analytical skills and background in natural language processing As most of the data is in German, good German skills are required PhD Position With Focus On Data Science And Networks MSc degree in physics, data science, computer science alternatively MSc MA in social science with exceptional skills in quantitative analysis strong background in data analysis and modelling excellent programming skills and ideally also high-performance computing and machine learning database management e.g., SQL, neo4j As most of the data is in German, good German skills are required Apart from excellent technical skills, both candidates should show an interest in social and political science questions, be able to communicate well in an interdisciplinary team, and be highly motivated and creative in what they do. ETH Zurich ETH Zurich is one of the world’s leading universities specialising in science and technology. We are renowned for our excellent education, cutting-edge fundamental research and direct transfer of new knowledge into society. Over 30,000 people from more than 120 countries find our university to be a place that promotes independent thinking and an environment that inspires excellence. Located in the heart of Europe, yet forging connections all over the world, we work together to develop solutions for the global challenges of today and tomorrow. Interested?We look forward to receiving your online application with the following documents: CV, motivation letter including detailed description of relevant skills and your interest in the above mentioned research project and transcript of grades. Please apply exclusively via the online application portal. Applications sent by email or post will not be considered.Further information about the Chair of Systems Design can be found on our website . For specific inquiries, please contact Prof. Dr. Frank Schweitzer at click apply no applications .Seniority levelAssociateEmployment typeTemporaryJob functionResearchAnalystInformation TechnologyIndustriesInformation Technology and ServicesResearchStaffing and Recruiting\"\n",
    "\n",
    "sent_scores = []\n",
    "for sentence in nltk.sent_tokenize(article_text):\n",
    "    score = 0\n",
    "    sent_tokens = tokenize_and_stem(sentence)\n",
    "    for token in (t for t in sent_tokens if t in feature_names):\n",
    "        score += tdm[article_id, feature_names.index(token)]\n",
    "    sent_scores.append((score / len(sent_tokens), sentence))\n",
    "\n",
    "summary_length = int(math.ceil(len(sent_scores) / 5))\n",
    "sent_scores.sort(key=lambda sent: sent[0], reverse=True)\n",
    "\n",
    "print ('*** SUMMARY ***')\n",
    "for summary_sentence in sent_scores[:summary_length]:\n",
    "    print (summary_sentence[1])\n",
    "\n",
    "print ('\\n*** ORIGINAL ***')\n",
    "print (article_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
